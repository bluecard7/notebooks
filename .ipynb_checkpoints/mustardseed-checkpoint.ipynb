{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.7.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CUDA-enabled GPUs: 1\n",
      "Current Device ID: 0\n",
      "Current Device Name: GeForce GTX 1650\n"
     ]
    }
   ],
   "source": [
    "device_cnt = torch.cuda.device_count()\n",
    "cur_device = torch.cuda.current_device()\n",
    "device_name = torch.cuda.get_device_name(cur_device)\n",
    "\n",
    "print(f\"Number of CUDA-enabled GPUs: {device_cnt}\\nCurrent Device ID: {cur_device}\\nCurrent Device Name: {device_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is downloaded from: https://www.kaggle.com/borismarjanovic/price-volume-data-for-all-us-stocks-etfs\n",
    "\n",
    "(just using the Stocks data, not ETF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stocks data exists\n"
     ]
    }
   ],
   "source": [
    "from fastai.data.all import file_extract\n",
    "from pathlib import Path\n",
    "\n",
    "if not Path('./Stocks').exists():\n",
    "    file_extract('./Stocks.zip', '.')\n",
    "else:\n",
    "    print('Stocks data exists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.data.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "files = get_files('./Stocks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#72) [Path('Stocks/a.us.txt'),Path('Stocks/adro.us.txt'),Path('Stocks/ahpa.us.txt'),Path('Stocks/amag.us.txt'),Path('Stocks/apf.us.txt'),Path('Stocks/asnd.us.txt'),Path('Stocks/axs_e.us.txt'),Path('Stocks/bctf.us.txt'),Path('Stocks/bkj.us.txt'),Path('Stocks/bpt.us.txt')...]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_files = files[::100]\n",
    "some_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# benchmark these\n",
    "\n",
    "# brute-force\n",
    "# def categorize_growth1(filepath):\n",
    "#     with filepath.open() as f:\n",
    "#         f.readline()\n",
    "#         first = last = f.readline()\n",
    "#         for cont in f:\n",
    "#             last = f.readline()\n",
    "#         return first, last\n",
    "        \n",
    "# read-back byte by byte, then readline when encountering a newline\n",
    "def categorize_growth2(filepath):\n",
    "    with filepath.open('rb+') as f:\n",
    "        f.readline()\n",
    "        first = f.readline().strip(b'\\r\\n')\n",
    "        f.seek(-2, 2)\n",
    "        while f.read(1) != b'\\n': f.seek(-2, 1)\n",
    "        return first, f.readline().strip(b'\\r\\n')\n",
    "\n",
    "# same as approach , but build last line as you read(probably have to reverse + join)\n",
    "# def categorize_growth3(filepath):\n",
    "#     with filepath.open('rb+') as f:\n",
    "#         f.readline()\n",
    "#         first = f.readline().strip(b'\\r\\n')\n",
    "#         f.seek(-2, 2)\n",
    "#         last_line = [f.read(1)]\n",
    "#         while last_line[-1] != b'\\n':\n",
    "#             f.seek(-2, 1)\n",
    "#             last_line.append(f.read(1))\n",
    "#         return first, b''.join(last_line)[::-1].strip(b'\\r\\n')\n",
    "\n",
    "# read-back byte by byte, then readline when encountering a newline (read more to be more efficient)\n",
    "# Potential issue: assumes to overread once\n",
    "def categorize_growth4(filepath):\n",
    "    with filepath.open('rb+') as f:\n",
    "        f.readline()\n",
    "        first = f.readline().strip(b'\\r\\n')\n",
    "        f.seek(-11, 2)\n",
    "        while b'\\n' not in f.read(10): f.seek(-20, 1)\n",
    "        f.seek(-10, 1)\n",
    "        f.readline()\n",
    "        return first, f.readline().strip(b'\\r\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# [categorize_growth2(filepath) for filepath in some_files]\n",
    "import os\n",
    "def nonempty_file(filepath): return os.path.getsize(filepath) != 0\n",
    "    \n",
    "map2 = dict()\n",
    "for filepath in filter(nonempty_file, some_files):\n",
    "    map2[filepath] = categorize_growth2(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "map4 = dict()\n",
    "for filepath in filter(nonempty_file, some_files):\n",
    "    map4[filepath] = categorize_growth4(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PASS] Stocks\\a.us.txt\n",
      "[PASS] Stocks\\adro.us.txt\n",
      "[PASS] Stocks\\ahpa.us.txt\n",
      "[PASS] Stocks\\amag.us.txt\n",
      "[PASS] Stocks\\apf.us.txt\n",
      "[PASS] Stocks\\asnd.us.txt\n",
      "[PASS] Stocks\\axs_e.us.txt\n",
      "[PASS] Stocks\\bctf.us.txt\n",
      "[PASS] Stocks\\bkj.us.txt\n",
      "[PASS] Stocks\\bpt.us.txt\n",
      "[PASS] Stocks\\bxmx.us.txt\n",
      "[PASS] Stocks\\cbs.us.txt\n",
      "[PASS] Stocks\\cffi.us.txt\n",
      "[PASS] Stocks\\civi.us.txt\n",
      "[PASS] Stocks\\cncr.us.txt\n",
      "[PASS] Stocks\\cpt.us.txt\n",
      "[PASS] Stocks\\ctw.us.txt\n",
      "[PASS] Stocks\\c_l.us.txt\n",
      "[PASS] Stocks\\dhr.us.txt\n",
      "[PASS] Stocks\\dspg.us.txt\n",
      "[PASS] Stocks\\ebtc.us.txt\n",
      "[PASS] Stocks\\eltk.us.txt\n",
      "[PASS] Stocks\\erii.us.txt\n",
      "[PASS] Stocks\\ewmc.us.txt\n",
      "[PASS] Stocks\\fet.us.txt\n",
      "[PASS] Stocks\\fmcir.us.txt\n",
      "[PASS] Stocks\\fss.us.txt\n",
      "[PASS] Stocks\\gdp.us.txt\n",
      "[PASS] Stocks\\glow.us.txt\n",
      "[PASS] Stocks\\grp-u.us.txt\n",
      "[PASS] Stocks\\hayn.us.txt\n",
      "[PASS] Stocks\\hii.us.txt\n",
      "[PASS] Stocks\\htd.us.txt\n",
      "[PASS] Stocks\\icsh.us.txt\n",
      "[PASS] Stocks\\ingn.us.txt\n",
      "[PASS] Stocks\\itek.us.txt\n",
      "[PASS] Stocks\\jmom.us.txt\n",
      "[PASS] Stocks\\ked.us.txt\n",
      "[PASS] Stocks\\kt.us.txt\n",
      "[PASS] Stocks\\lilak.us.txt\n",
      "[PASS] Stocks\\ltxb.us.txt\n",
      "[PASS] Stocks\\mcv.us.txt\n",
      "[PASS] Stocks\\mik.us.txt\n",
      "[PASS] Stocks\\mpo.us.txt\n",
      "[PASS] Stocks\\muj.us.txt\n",
      "[PASS] Stocks\\ndrm.us.txt\n",
      "[PASS] Stocks\\nmfc.us.txt\n",
      "[PASS] Stocks\\nuan.us.txt\n",
      "[PASS] Stocks\\obe.us.txt\n",
      "[PASS] Stocks\\ophc.us.txt\n",
      "[PASS] Stocks\\pbi.us.txt\n",
      "[PASS] Stocks\\pfl.us.txt\n",
      "[PASS] Stocks\\pnc-ws.us.txt\n",
      "[PASS] Stocks\\psf.us.txt\n",
      "[PASS] Stocks\\qumu.us.txt\n",
      "[PASS] Stocks\\rfi.us.txt\n",
      "[PASS] Stocks\\rop.us.txt\n",
      "[PASS] Stocks\\san_b.us.txt\n",
      "[PASS] Stocks\\self.us.txt\n",
      "[PASS] Stocks\\skt.us.txt\n",
      "[PASS] Stocks\\sol.us.txt\n",
      "[PASS] Stocks\\susb.us.txt\n",
      "[PASS] Stocks\\tcrd.us.txt\n",
      "[PASS] Stocks\\tli.us.txt\n",
      "[PASS] Stocks\\trup.us.txt\n",
      "[PASS] Stocks\\ucbi.us.txt\n",
      "[PASS] Stocks\\utmd.us.txt\n",
      "[PASS] Stocks\\vmw.us.txt\n",
      "[PASS] Stocks\\wbig.us.txt\n",
      "[PASS] Stocks\\wnrl.us.txt\n",
      "[PASS] Stocks\\xlrn.us.txt\n"
     ]
    }
   ],
   "source": [
    "if len(map2) != len(map4): print('UNEQUAL')\n",
    "for k,v in map2.items():\n",
    "    if map4[k] == v:\n",
    "        print('[PASS]', k)\n",
    "    else: \n",
    "        print('[FAIL]', v, '- got', map4[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.032823200000001\n",
      "0.9754511000000008\n"
     ]
    }
   ],
   "source": [
    "# benchmark\n",
    "import timeit\n",
    "# print(timeit.timeit(\"categorize_growth1(some_files[0])\", globals=locals(), number=10000)) ~ 10 s\n",
    "print(timeit.timeit(\"categorize_growth2(some_files[0])\", globals=locals(), number=10000))\n",
    "# print(timeit.timeit(\"categorize_growth3(some_files[0])\", globals=locals(), number=10000)) ~ 2.841 s, very similar to approach 2\n",
    "print(timeit.timeit(\"categorize_growth4(some_files[0])\", globals=locals(), number=10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'categorize_growth1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-169f5de0fb63>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#     blocks = (, CategoryBlock),\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mget_items\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_files\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mget_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcategorize_growth1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;31m#     item_tfms = []\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0msplitter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomSplitter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalid_pct\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'categorize_growth1' is not defined"
     ]
    }
   ],
   "source": [
    "DataBlock(\n",
    "#     blocks = (, CategoryBlock),\n",
    "    get_items = get_files,\n",
    "    get_y = categorize_growth4,\n",
    "#     item_tfms = []\n",
    "    splitter = RandomSplitter(valid_pct=0.2, seed=42)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (fastai)",
   "language": "python",
   "name": "learn-fastai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
